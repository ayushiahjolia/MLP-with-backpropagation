{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2) #setting seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./train_data.csv\", header=None)\n",
    "Y = pd.read_csv(\"./train_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data - 70% and Validation data - 30%\n",
    "X_train = X.iloc[0:17327,:]\n",
    "y_train = Y.iloc[0:17327,:]\n",
    "X_val = X.iloc[17327:,:]\n",
    "y_val = Y.iloc[17327:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.T)\n",
    "y_train = np.array(y_train.T)\n",
    "X_val = np.array(X_val.T)\n",
    "y_val = np.array(y_val.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (784, 17327)\n",
      "The shape of y_train is: (4, 17327)\n",
      "The shape of X_val is: (784, 7427)\n",
      "The shape of y_val is: (4, 7427)\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('The shape of X_val is: ' + str(X_val.shape))\n",
    "print ('The shape of y_val is: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized form :\n",
    "- We have used vectorized implementation because it is computationally faster.\n",
    "\n",
    "### Loss/Error Function :\n",
    "- We have used cross-entropy loss function to compute errors.\n",
    "- Cross-entropy loss is a loss function widely used for multi-class classification problems. \n",
    "- Itâ€™s built on the concept of cross-entropy, which is a measurement of two probability vectors (often with one of them being one-hot encoded labels in datasets)\n",
    "\n",
    "### Activation function : \n",
    "- We use non linear activation function because it helps learning easier for next layer. \n",
    "- If we were to use liner activation function in the hidden layer then it will compute the linear equation for all the layer thus there won't be any learning during the process.\n",
    "- **Sigmoid** : \n",
    "    - We have used sigmoid function between input and hidden layer.\n",
    "    - It takes real values as input and gives probability between 0 and 1 as output.\n",
    "    - This may cause issue of vanishing gradient, which we have solved further.\n",
    "- **Softmax** :\n",
    "    - We have used softmax function between hidden and output layer.\n",
    "    - As we are solving multiclass classification problem, softmax is best choice to get probability based results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines number of neurons in each layer\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x=X.shape[0]\n",
    "    n_h=40\n",
    "    n_y=Y.shape[0]\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializes random values for weights and bias\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    W1=np.random.randn(n_h, n_x) * np.sqrt(2/(n_x + n_h))\n",
    "    b1=np.zeros((n_h, 1))\n",
    "    W2=np.random.randn(n_y, n_h) * np.sqrt(2/(n_y + n_h))\n",
    "    b2=np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                \"b1\": b1,\n",
    "                \"W2\": W2,\n",
    "                \"b2\": b2 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the issue of vanishing gradient, we multiplied random initialized weights with sqrt(2/(input_neurons + hiddenlayer_neurons))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate outputs using feedforward\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1= parameters[\"W1\"]\n",
    "    b1= parameters[\"b1\"]\n",
    "    W2= parameters[\"W2\"]\n",
    "    b2= parameters[\"b2\"]\n",
    "    \n",
    "    Z1= np.dot(W1,X) + b1 \n",
    "    A1= sigmoid(Z1)\n",
    "    Z2= np.dot(W2,A1) + b2\n",
    "    A2= softmax(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "            \"A1\": A1,\n",
    "            \"Z2\": Z2,\n",
    "            \"A2\": A2 }\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes cost using Cross entropy loss function\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (1/m)*np.sum(-Y * np.log(A2))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives of weights and biases\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):  \n",
    "    \n",
    "    m = X.shape[1] \n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dz2 = (1/m) * np.subtract(A2,Y) \n",
    "    dW2 = np.dot(dz2,A1.T)  \n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True)\n",
    "    dz1 = (np.dot(W2.T,dz2))*(A1 - np.power(A1, 2)) \n",
    "    dW1 = np.dot(dz1,X.T)\n",
    "    db1 = np.sum(dz1,axis=1, keepdims=True)\n",
    "    \n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "    \"db1\": db1,\n",
    "    \"dW2\": dW2,\n",
    "    \"db2\": db2 }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights using gradient descent algo\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.05):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "\n",
    "    dW1=grads[\"dW1\"]\n",
    "    db1=grads[\"db1\"]\n",
    "    dW2=grads[\"dW2\"]\n",
    "    db2=grads[\"db2\"]\n",
    "    \n",
    "    W1= W1 - (learning_rate*dW1)\n",
    "    b1= b1 - (learning_rate*db1)\n",
    "    W2= W2 - (learning_rate*dW2)\n",
    "    b2= b2 - (learning_rate*db2)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "    \"b1\": b1,\n",
    "    \"W2\": W2,\n",
    "    \"b2\": b2 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried different learning rates and 0.05 is giving best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_parameters, X):\n",
    "    A2, cache = forward_propagation(X, pred_parameters)\n",
    "    predictions = np.argmax(A2, axis=0) \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to one-hot encodings\n",
    "def onehot_encoded(y):   \n",
    "    my_list = list()\n",
    "    for i in y:\n",
    "        temp = [0, 0, 0, 0]\n",
    "        temp[i] = 1\n",
    "        my_list.append(temp)\n",
    "        \n",
    "    return np.array(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acc_calc import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network - combining all functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of neural network\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    accuracy_matrix = []\n",
    "    \n",
    "    for i in range(0, num_iterations+1):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        preds2 = predict(parameters, X_val)\n",
    "        preds = onehot_encoded(preds2)\n",
    "        accuracy_val = accuracy(y_val.T, preds)\n",
    "        accuracy_matrix.append(accuracy_val)\n",
    "        print(\"Accuracy after iteration %i: %f\" %(i, accuracy_val)) \n",
    "\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "    return parameters, accuracy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration 0: 0.281944\n",
      "Accuracy after iteration 1: 0.297294\n",
      "Accuracy after iteration 2: 0.314259\n",
      "Accuracy after iteration 3: 0.330416\n",
      "Accuracy after iteration 4: 0.344150\n",
      "Accuracy after iteration 5: 0.363404\n",
      "Accuracy after iteration 6: 0.397738\n",
      "Accuracy after iteration 7: 0.443113\n",
      "Accuracy after iteration 8: 0.492931\n",
      "Accuracy after iteration 9: 0.542615\n",
      "Accuracy after iteration 10: 0.586105\n",
      "Accuracy after iteration 11: 0.623805\n",
      "Accuracy after iteration 12: 0.659351\n",
      "Accuracy after iteration 13: 0.686011\n",
      "Accuracy after iteration 14: 0.710920\n",
      "Accuracy after iteration 15: 0.730039\n",
      "Accuracy after iteration 16: 0.743907\n",
      "Accuracy after iteration 17: 0.758584\n",
      "Accuracy after iteration 18: 0.769759\n",
      "Accuracy after iteration 19: 0.779992\n",
      "Accuracy after iteration 20: 0.788474\n",
      "Accuracy after iteration 21: 0.799515\n",
      "Accuracy after iteration 22: 0.803959\n",
      "Accuracy after iteration 23: 0.810825\n",
      "Accuracy after iteration 24: 0.817558\n",
      "Accuracy after iteration 25: 0.822539\n",
      "Accuracy after iteration 26: 0.825905\n",
      "Accuracy after iteration 27: 0.829945\n",
      "Accuracy after iteration 28: 0.834253\n",
      "Accuracy after iteration 29: 0.838293\n",
      "Accuracy after iteration 30: 0.840582\n",
      "Accuracy after iteration 31: 0.843275\n",
      "Accuracy after iteration 32: 0.847179\n",
      "Accuracy after iteration 33: 0.849334\n",
      "Accuracy after iteration 34: 0.851622\n",
      "Accuracy after iteration 35: 0.853911\n",
      "Accuracy after iteration 36: 0.856335\n",
      "Accuracy after iteration 37: 0.859297\n",
      "Accuracy after iteration 38: 0.861182\n",
      "Accuracy after iteration 39: 0.862259\n",
      "Accuracy after iteration 40: 0.865087\n",
      "Accuracy after iteration 41: 0.867241\n",
      "Accuracy after iteration 42: 0.868588\n",
      "Accuracy after iteration 43: 0.870742\n",
      "Accuracy after iteration 44: 0.872223\n",
      "Accuracy after iteration 45: 0.873704\n",
      "Accuracy after iteration 46: 0.874512\n",
      "Accuracy after iteration 47: 0.875589\n",
      "Accuracy after iteration 48: 0.876801\n",
      "Accuracy after iteration 49: 0.877878\n",
      "Accuracy after iteration 50: 0.878551\n",
      "Accuracy after iteration 51: 0.879763\n",
      "Accuracy after iteration 52: 0.880571\n",
      "Accuracy after iteration 53: 0.881648\n",
      "Accuracy after iteration 54: 0.882456\n",
      "Accuracy after iteration 55: 0.883937\n",
      "Accuracy after iteration 56: 0.884745\n",
      "Accuracy after iteration 57: 0.885957\n",
      "Accuracy after iteration 58: 0.887034\n",
      "Accuracy after iteration 59: 0.887707\n",
      "Accuracy after iteration 60: 0.889457\n",
      "Accuracy after iteration 61: 0.890131\n",
      "Accuracy after iteration 62: 0.891342\n",
      "Accuracy after iteration 63: 0.892016\n",
      "Accuracy after iteration 64: 0.893227\n",
      "Accuracy after iteration 65: 0.894439\n",
      "Accuracy after iteration 66: 0.895651\n",
      "Accuracy after iteration 67: 0.896728\n",
      "Accuracy after iteration 68: 0.898209\n",
      "Accuracy after iteration 69: 0.899017\n",
      "Accuracy after iteration 70: 0.899960\n",
      "Accuracy after iteration 71: 0.900767\n",
      "Accuracy after iteration 72: 0.901037\n",
      "Accuracy after iteration 73: 0.901575\n",
      "Accuracy after iteration 74: 0.901979\n",
      "Accuracy after iteration 75: 0.902383\n",
      "Accuracy after iteration 76: 0.903191\n",
      "Accuracy after iteration 77: 0.903730\n",
      "Accuracy after iteration 78: 0.904134\n",
      "Accuracy after iteration 79: 0.904537\n",
      "Accuracy after iteration 80: 0.905211\n",
      "Accuracy after iteration 81: 0.905749\n",
      "Accuracy after iteration 82: 0.906019\n",
      "Accuracy after iteration 83: 0.906826\n",
      "Accuracy after iteration 84: 0.907634\n",
      "Accuracy after iteration 85: 0.907904\n",
      "Accuracy after iteration 86: 0.908577\n",
      "Accuracy after iteration 87: 0.908846\n",
      "Accuracy after iteration 88: 0.909250\n",
      "Accuracy after iteration 89: 0.909789\n",
      "Accuracy after iteration 90: 0.910058\n",
      "Accuracy after iteration 91: 0.910462\n",
      "Accuracy after iteration 92: 0.910596\n",
      "Accuracy after iteration 93: 0.910866\n",
      "Accuracy after iteration 94: 0.911135\n",
      "Accuracy after iteration 95: 0.911539\n",
      "Accuracy after iteration 96: 0.911943\n",
      "Accuracy after iteration 97: 0.912347\n",
      "Accuracy after iteration 98: 0.912751\n",
      "Accuracy after iteration 99: 0.913424\n",
      "Accuracy after iteration 100: 0.913693\n",
      "Accuracy after iteration 101: 0.913828\n",
      "Accuracy after iteration 102: 0.914232\n",
      "Accuracy after iteration 103: 0.914636\n",
      "Accuracy after iteration 104: 0.915174\n",
      "Accuracy after iteration 105: 0.915578\n",
      "Accuracy after iteration 106: 0.915444\n",
      "Accuracy after iteration 107: 0.915848\n",
      "Accuracy after iteration 108: 0.916386\n",
      "Accuracy after iteration 109: 0.916521\n",
      "Accuracy after iteration 110: 0.917194\n",
      "Accuracy after iteration 111: 0.917867\n",
      "Accuracy after iteration 112: 0.918540\n",
      "Accuracy after iteration 113: 0.919079\n",
      "Accuracy after iteration 114: 0.919618\n",
      "Accuracy after iteration 115: 0.920291\n",
      "Accuracy after iteration 116: 0.920695\n",
      "Accuracy after iteration 117: 0.921368\n",
      "Accuracy after iteration 118: 0.921907\n",
      "Accuracy after iteration 119: 0.921907\n",
      "Accuracy after iteration 120: 0.922176\n",
      "Accuracy after iteration 121: 0.922310\n",
      "Accuracy after iteration 122: 0.922849\n",
      "Accuracy after iteration 123: 0.923118\n",
      "Accuracy after iteration 124: 0.923388\n",
      "Accuracy after iteration 125: 0.923792\n",
      "Accuracy after iteration 126: 0.923792\n",
      "Accuracy after iteration 127: 0.924196\n",
      "Accuracy after iteration 128: 0.925003\n",
      "Accuracy after iteration 129: 0.925003\n",
      "Accuracy after iteration 130: 0.925138\n",
      "Accuracy after iteration 131: 0.926215\n",
      "Accuracy after iteration 132: 0.926619\n",
      "Accuracy after iteration 133: 0.927023\n",
      "Accuracy after iteration 134: 0.927023\n",
      "Accuracy after iteration 135: 0.927292\n",
      "Accuracy after iteration 136: 0.927292\n",
      "Accuracy after iteration 137: 0.927427\n",
      "Accuracy after iteration 138: 0.927831\n",
      "Accuracy after iteration 139: 0.927831\n",
      "Accuracy after iteration 140: 0.928235\n",
      "Accuracy after iteration 141: 0.928235\n",
      "Accuracy after iteration 142: 0.928504\n",
      "Accuracy after iteration 143: 0.928504\n",
      "Accuracy after iteration 144: 0.928773\n",
      "Accuracy after iteration 145: 0.928773\n",
      "Accuracy after iteration 146: 0.928908\n",
      "Accuracy after iteration 147: 0.929043\n",
      "Accuracy after iteration 148: 0.929581\n",
      "Accuracy after iteration 149: 0.930120\n",
      "Accuracy after iteration 150: 0.929985\n"
     ]
    }
   ],
   "source": [
    "parameters, accuracy_matrix = nn_model(X_train, y_train, 32, num_iterations=150, print_cost=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 150 epochs, and didn't increase to avoid overfitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,151), accuracy_matrix)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving weights to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('weights.pkl', 'wb+')\n",
    "pickle.dump(parameters, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
